{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-dotenv\n",
        "# !pip install langchain\n",
        "# !pip install langchain-google-genai\n",
        "# !pip install google-generativeai\n",
        "# !pip install langgraph\n",
        "# !pip install langchain-community\n",
        "# !pip install duckduckgo_search\n",
        "# !pip install tavily-python\n",
        "# !pip install langgraph-checkpoint-sqlite"
      ],
      "metadata": {
        "id": "Jdodw6sN4lGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "import os\n",
        "import getpass\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "import re\n",
        "from duckduckgo_search import DDGS\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "from tavily import TavilyClient\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.chains import SequentialChain\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "\n",
        "load_dotenv(find_dotenv())"
      ],
      "metadata": {
        "id": "Pwmec_bO-dDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b55fd4-ca3b-482c-86fc-d1fd265393b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API Key\")"
      ],
      "metadata": {
        "id": "Kh3fGEYTLXtR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "668f700a-1342-4b8a-b136-e2d99021b1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provide your Google API Key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"TAVILY_API_KEY\" not in os.environ:\n",
        "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Provide your Tavily API Key\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAGvGoel_Bjb",
        "outputId": "3ab0a1f0-a616-4cc4-bdaf-a7b9df129ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provide your Tavily API Key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "YPibOxplMTY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Langchain** is a framework for developing applications powered by large language model. It is an open source ochestration framework for the development of applications that uses LLM"
      ],
      "metadata": {
        "id": "96Ken1_y_qsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a generic interface for any LLM, abstracting the complexities of interacting with different LLMs"
      ],
      "metadata": {
        "id": "9rRLkqptMkV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chains, in langchain, are core of the langchain work flow. It combine LLMs with other components, creating applications by executing a sequence of functions"
      ],
      "metadata": {
        "id": "hNceZ00xM03S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Models"
      ],
      "metadata": {
        "id": "wuYhB2dOMuFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chat models** is a language model that uses chat messages as input and return chat messages as outputs."
      ],
      "metadata": {
        "id": "HuAZrfKBMwPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "result = llm.invoke(\"What is 2 + 2?\")\n",
        "print(f\"Response: {result.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwyi1sHrMhVL",
        "outputId": "ab0f9e57-0ed0-42ed-8a7f-c6b645c79091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Template\n",
        "It takes a prompt as input, which is set of instructions or input provided by user to guie model's response. It helps generate relevant output and understanding the context"
      ],
      "metadata": {
        "id": "UBCOOWvwPrew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt template** class formalizes the composition of prompts without the need to manually hard code context and queries"
      ],
      "metadata": {
        "id": "CfDxqcFZP20E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Translate the text delimited by triple back ticks into\n",
        "style that is {style}\n",
        "\n",
        "text: ```{text}```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IwbOvz-bQmXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "print(f\"Prompt template passed to the model:\\n{prompt_template.messages[0].prompt.template}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFQwZHafN6i_",
        "outputId": "695d1c6f-60db-44ab-d84d-965c9676d243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt template passed to the model:\n",
            " \n",
            "Translate the text delimited by triple back ticks into\n",
            "style that is {style}\n",
            "\n",
            "text: ```{text}```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Inputs in the prompt: {prompt_template.messages[0].prompt.input_variables}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y19U0aVsReWm",
        "outputId": "d7981ae9-4966-454f-8c84-f71535fe68a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs in the prompt: ['style', 'text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_email = \"Yeh kya hae bhai? Yeh woh cheez nahi hae jo mein nein \\\n",
        "order ki thi. Tasweer mein yeh buhat muhktalif tha. Abb mein kya karoon? \\\n",
        "Replacment or return ki kya policy hae?\"\n",
        "\n",
        "style = \"American style that is polite, calm and respectful in tone\""
      ],
      "metadata": {
        "id": "8W19FkzRQ_Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_message = prompt_template.format_messages(style=style,\n",
        "                                                   text=customer_email)\n",
        "\n",
        "print(f\"Customer message: {customer_message[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrLkAV8ARDXO",
        "outputId": "70a41c8c-e336-4e62-90cc-2fd6deafe807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer message: content=' \\nTranslate the text delimited by triple back ticks into\\nstyle that is American style that is polite, calm and respectful in tone\\n\\ntext: ```Yeh kya hae bhai? Yeh woh cheez nahi hae jo mein nein order ki thi. Tasweer mein yeh buhat muhktalif tha. Abb mein kya karoon? Replacment or return ki kya policy hae?```\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(customer_message)\n",
        "print(f\"Model response: {response.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkK37WxRR9eA",
        "outputId": "80adcf11-a34e-40a6-8de2-c2d29da6ccb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response: Excuse me, this is not what I ordered. It looks very different from the picture. What should I do now? What is your policy on replacements or returns?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_reply = \"\"\"\n",
        "Thanks for reaching out. We are sorry for inconvenience. \\\n",
        "We upload high quality pictures that best reflects the \\\n",
        "product quality. You can replace the item if it is ordered \\\n",
        "within 7 days. You cannot return the item. Our team will check \\\n",
        "the item and tell you what to do next. Thanks for understanding\n",
        "\"\"\"\n",
        "\n",
        "service_language = \"\"\"\n",
        "Translate the text into Urdu words using polite\n",
        "tone.\n",
        "\"\"\"\n",
        "\n",
        "service_message = prompt_template.format_messages(style=service_language,\n",
        "                                                  text=service_reply)\n",
        "\n",
        "\n",
        "service_response = llm.invoke(service_message)\n",
        "print(service_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqyqTogESOqC",
        "outputId": "2ab7d73a-e6df-492f-ca22-4e548bdd9048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "شکریہ کہ ہمارے پاس پہنچے۔ ہم آپ کو ہونے والی پریشانی پر معذرت خواہ ہیں۔ ہم بہترین معیار کی تصاویر اپ لوڈ کرتے ہیں جو مصنوعات کے معیار کی عکاسی کرتی ہیں۔ اگر آپ نے 7 دنوں کے اندر آرڈر کیا ہے تو آپ اس آئٹم کو تبدیل کر سکتے ہیں۔ آپ اس آئٹم کو واپس نہیں کر سکتے۔ ہماری ٹیم آئٹم چیک کرے گی اور آپ کو بتائے گی کہ اس کے بعد کیا کرنا ہے۔ سمجھنے کے لئے شکریہ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see from the above example that instead of creating template for each response, we can create a template and reuse it"
      ],
      "metadata": {
        "id": "xlCEhM3oT5m2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory\n",
        "Traditionally, large language models (LLMS) doesn't have any long term memory of prior conversation unless you pass the whole chat history as an input. Langchain solve this problem with simple utilities for adding in memory into your application"
      ],
      "metadata": {
        "id": "CKlN72RuYHZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we alk about memory in langchain, we are referring to how the system remember the information from previous interactions"
      ],
      "metadata": {
        "id": "fEZU-a7uYpkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user = \"Hello! My name is Maaz\"\n",
        "response = llm.invoke(user)\n",
        "print(f\"User: {user}\\nResponse: {response.content}\")\n",
        "\n",
        "user = \"What is 2 + 2?\"\n",
        "response = llm.invoke(user)\n",
        "print(f\"User: {user}\\nResponse: {response.content}\")\n",
        "\n",
        "user = \"What is my name?\"\n",
        "response = llm.invoke(user)\n",
        "print(f\"User: {user}\\nResponse: {response.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WmKSxTtZDMe",
        "outputId": "5c1eec01-1583-47c3-e95a-1b9fd3313104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hello! My name is Maaz\n",
            "Response: Hello Maaz! It's great to meet you. I am Gemini, a multimodal AI language model. I am designed to understand and generate human language, and to answer your questions and assist you with a wide range of tasks. Is there anything I can help you with today?\n",
            "User: What is 2 + 2?\n",
            "Response: 4\n",
            "User: What is my name?\n",
            "Response: I do not have access to your personal information, including your name, as I am a chatbot assistant and do not have a physical presence or the ability to interact with the real world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the model didn't remember my name despite being recently shared. The response of LLM model is stateless"
      ],
      "metadata": {
        "id": "hHLNjNJcZpu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Buffer Memory"
      ],
      "metadata": {
        "id": "MfEssywvYytM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It stores the entire conversation history from beginning to end"
      ],
      "metadata": {
        "id": "As62gV_qZz0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "conversation.predict(input = \"My name is Maaz\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "iHmfw6MzTCC9",
        "outputId": "ba6da1d4-c1c1-442b-dbb6-32c331aa37b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: My name is Maaz\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Maaz, my name is Gemini. How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is 2 + 2?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "vEk8H5QSarBx",
        "outputId": "55631cd8-f8de-46ae-c6c2-8363cd411491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: My name is Maaz\n",
            "AI: Hello Maaz, my name is Gemini. How can I help you today?\n",
            "Human: What is 2 + 2?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2 + 2 is 4.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "rCieNyEya5GT",
        "outputId": "6f81dd75-aba8-4c9a-cda4-0f680ae28ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: My name is Maaz\n",
            "AI: Hello Maaz, my name is Gemini. How can I help you today?\n",
            "Human: What is 2 + 2?\n",
            "AI: 2 + 2 is 4.\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Maaz, as you told me earlier.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Entire conversation: \\n{memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJDhwgBKbqdL",
        "outputId": "6bca9698-3b2c-4ad4-d76f-55eb9bd52028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entire conversation: \n",
            "Human: My name is Maaz\n",
            "AI: Hello Maaz, my name is Gemini. How can I help you today?\n",
            "Human: What is 2 + 2?\n",
            "AI: 2 + 2 is 4.\n",
            "Human: What is my name?\n",
            "AI: Your name is Maaz, as you told me earlier.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAPWeQptb2F-",
        "outputId": "0e510c2c-1e00-416f-8a98-3dcd6ac73cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: My name is Maaz\\nAI: Hello Maaz, my name is Gemini. How can I help you today?\\nHuman: What is 2 + 2?\\nAI: 2 + 2 is 4.\\nHuman: What is my name?\\nAI: Your name is Maaz, as you told me earlier.'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Buffer Window Memory"
      ],
      "metadata": {
        "id": "TPkrG0EkbF46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It stores only the most recent exchanges within defined window size. Instead of saving entire conversation, it focuses on recent conversation"
      ],
      "metadata": {
        "id": "ySvKf83hbMr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is useful when the goal is to make AI focus on recent chat without overwheling by too much information"
      ],
      "metadata": {
        "id": "G2zaqSjcbdo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=1) #remember only 1 recent message\n",
        "\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "\n",
        "print(f\"Conversations: \\n{memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FInsACIKa_ux",
        "outputId": "13333a66-e0b4-49e1-ce74-2ac57aabb734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversations: \n",
            "Human: Not much, just hanging\n",
            "AI: Cool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "memory = ConversationBufferWindowMemory(k=2) #remember only 2 recent message\n",
        "\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"How is the weather\"},\n",
        "                    {\"output\": \"Hot\"})\n",
        "\n",
        "print(f\"Conversations: \\n{memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgzczFJ8chWG",
        "outputId": "3561b61d-cc75-4b9e-e8ea-4efe9d687484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversations: \n",
            "Human: Not much, just hanging\n",
            "AI: Cool\n",
            "Human: How is the weather\n",
            "AI: Hot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Token Buffer Memory"
      ],
      "metadata": {
        "id": "ySUXIGpScyHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the conversation buffer memory but instead of counting interactions, it count tokens"
      ],
      "metadata": {
        "id": "9fXiU-frc36p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This memory type limits how much the AI remembers based on number of tokens. Once the token limit is reached, the oldest tokens are forgotten"
      ],
      "metadata": {
        "id": "7y5jM0QTdCCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "\n",
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=40)\n",
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})\n",
        "\n",
        "print(f\"Conversations: {memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLvzwJmzcpV3",
        "outputId": "4ba63da7-4753-4d12-f038-b1ba8d9d3b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversations: Human: AI is what?!\n",
            "AI: Amazing!\n",
            "Human: Backpropagation is what?\n",
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"How is the weather?\"},\n",
        "                    {\"output\": \"Windy and cold!\"})\n",
        "print(f\"Memory: \\n{memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbPleG1OdOQS",
        "outputId": "a395ba4d-f27d-4694-8fbe-aca82897af1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory: \n",
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n",
            "Human: How is the weather?\n",
            "AI: Windy!\n",
            "Human: How is the weather?\n",
            "AI: Windy and cold!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Summary Memory"
      ],
      "metadata": {
        "id": "Sfx59OHKk0xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It doesn't store every detail but creates a summary of past interactions. This allows the model to remember what is important without needing full conversation"
      ],
      "metadata": {
        "id": "qgnnXVR6k6vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})\n",
        "\n",
        "print(f\"Conversation in memory: \\n{memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap7zD2BidseF",
        "outputId": "5a01dc6d-72d1-443a-9f77-c8e693441dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversation in memory: \n",
            "System: The human says hello. The AI asks what's up. The human says they are hanging. The AI says that's cool. The human asks what is on the schedule today.\n",
            "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(llm=llm,\n",
        "                                 memory=memory,\n",
        "                                 verbose=True)\n",
        "\n",
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "rsOHsKcHlJpl",
        "outputId": "d5c952cd-7b53-4e25-94c9-f53afc6884c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human says hello. The AI asks what's up. The human says they are hanging. The AI says that's cool. The human asks what is on the schedule today.\n",
            "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
            "Human: What would be a good demo to show?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I recommend the latest demo for LLM that allows the model to generate text in the voice of a specific person, including celebrities.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"New memory: {memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j6-7YKElpl5",
        "outputId": "535cbe9c-071d-4b57-94ee-cbbf31eba39c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New memory: System: The human says hello. The AI asks what's up. The human says they are hanging. The AI says that's cool. The human asks what is on the schedule today. The AI says there is a meeting at 8am with the product team, work on the LangChain project from 9am-12pm, and lunch at an Italian restaurant with a customer at Noon.\n",
            "Human: What would be a good demo to show?\n",
            "AI: I recommend the latest demo for LLM that allows the model to generate text in the voice of a specific person, including celebrities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains"
      ],
      "metadata": {
        "id": "JIBHodYnsTWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In langchain, **chains** are essential worflows that connect different steps or components allowing to process information in structured way"
      ],
      "metadata": {
        "id": "VW0HdL8bsXOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each step can involve tasks like\n",
        "\n",
        "*  Making call to language model\n",
        "*  Running a function\n",
        "*  Making decision based on input\n"
      ],
      "metadata": {
        "id": "T8PSAVgQ1EWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "They are quite powerful because they enable us to build complex, multiple interaction with language model in a modular and reusable way"
      ],
      "metadata": {
        "id": "Be4Qknax1SNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Sequential Chain\n",
        "It is a straightforward type of chain that involves sequence of steps where output of one step become input of another"
      ],
      "metadata": {
        "id": "mXIlBcGd1e9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chain executes each step in an order and doesn't involve amy branching or decision making"
      ],
      "metadata": {
        "id": "NVkR4hrM1pzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"The email should be about {email_topic}?\"\n",
        ")\n",
        "first_chain = LLMChain(llm=llm, prompt=first_prompt)\n",
        "\n",
        "print(f\"First chain: \\n{first_chain}\")\n",
        "print(f\"\\nFirst chain input variables: \\n{first_chain.prompt.input_variables}\")\n",
        "print(f\"\\nFirst chain input variables: \\n{first_chain.prompt.messages}\")"
      ],
      "metadata": {
        "id": "ALgA86nylszy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ed3f12-6932-4e4c-ffd7-c6831fca0412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First chain: \n",
            "prompt=ChatPromptTemplate(input_variables=['email_topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['email_topic'], template='The email should be about {email_topic}?'))]) llm=ChatGoogleGenerativeAI(model='models/gemini-pro', client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7f6f63bdf9a0>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x7f6f63bdf3d0>, default_metadata=())\n",
            "\n",
            "First chain input variables: \n",
            "['email_topic']\n",
            "\n",
            "First chain input variables: \n",
            "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['email_topic'], template='The email should be about {email_topic}?'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 20 word email on the email subject \\\n",
        "    {email_subject}\"\n",
        ")\n",
        "\n",
        "second_chain = LLMChain(llm=llm,\n",
        "                        prompt=second_prompt)\n",
        "\n",
        "print(f\"Second chain: \\n{second_chain}\")\n",
        "print(f\"\\nSecond chain input variables: \\n{second_chain.prompt.input_variables}\")\n",
        "print(f\"\\nSecond chain input variables: \\n{second_chain.prompt.messages}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um60bzyrsjf4",
        "outputId": "d2b7588b-0ba8-486f-a2a6-a539867061be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second chain: \n",
            "prompt=ChatPromptTemplate(input_variables=['email_subject'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['email_subject'], template='Write a 20 word email on the email subject     {email_subject}'))]) llm=ChatGoogleGenerativeAI(model='models/gemini-pro', client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7f6f63bdf9a0>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x7f6f63bdf3d0>, default_metadata=())\n",
            "\n",
            "Second chain input variables: \n",
            "['email_subject']\n",
            "\n",
            "Second chain input variables: \n",
            "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['email_subject'], template='Write a 20 word email on the email subject     {email_subject}'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simple_chain = SimpleSequentialChain(chains=[first_chain, second_chain],\n",
        "                                     verbose=False)\n",
        "\n",
        "topic = 'Sick leave request to the manager'\n",
        "print(f\"Model response: \\n{simple_chain.run(topic)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCsEEKTyuwHW",
        "outputId": "2068f715-92f5-4d64-93b6-b79185012566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response: Subject: Sick Leave Request\n",
            "\n",
            "Dear [Manager's Name],\n",
            "\n",
            "I request sick leave from [start date] to [end date] due to [symptoms]. I've attached a doctor's note and arranged coverage with [coworker's name]. I'll stay updated and provide recovery updates.\n",
            "\n",
            "Thanks,\n",
            "[Your Name]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential Chain"
      ],
      "metadata": {
        "id": "MmPex5H18LAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequential chain** is similar to simple sequential chain but more flexible and powerful."
      ],
      "metadata": {
        "id": "6BZjbu5O80M_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It consists of multiple steps that are executed in a sequence. Each step can involve more complex operation such as combining output, branching, or integrating different type of models"
      ],
      "metadata": {
        "id": "xwNk9v8Z9Ese"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outout of one step is still passed to the next but can include different operations beyond just passing information"
      ],
      "metadata": {
        "id": "f6Q63AnE9UB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
        "                             temperature=0)\n",
        "\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the review in English: {review}\"\n",
        ")\n",
        "first_chain = LLMChain(llm=llm,\n",
        "                       prompt=first_prompt,\n",
        "                       output_key=\"english_review\"\n",
        "                       )\n",
        "\n",
        "print(f\"First Chain: \\n{first_chain}\")\n",
        "print(f\"First chain input variables: \\n{first_chain.prompt.input_variables}\")\n",
        "print(f\"First chain output variables: \\n{first_chain.output_key}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib0TJEdmvPJS",
        "outputId": "08ac9f11-861e-4459-c2ca-365cc3de8abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Chain: \n",
            "prompt=ChatPromptTemplate(input_variables=['review'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['review'], template='Translate the review in English: {review}'))]) llm=ChatGoogleGenerativeAI(model='models/gemini-pro', temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7f6f60078340>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x7f6f6007b6d0>, default_metadata=()) output_key='english_review'\n",
            "First chain input variables: \n",
            "['review']\n",
            "First chain output variables: \n",
            "english_review\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Summarize the review in 1 sentence: {english_review}\"\n",
        ")\n",
        "\n",
        "second_chain = LLMChain(llm=llm,\n",
        "                        prompt=second_prompt,\n",
        "                        output_key=\"summary\"\n",
        "                        )\n",
        "\n",
        "print(f\"Second Chain: \\n{second_chain}\")\n",
        "print(f\"Second chain input variables: \\n{second_chain.prompt.input_variables}\")\n",
        "print(f\"Second chain output variables: \\n{second_chain.output_key}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw_BNup_4bLI",
        "outputId": "7b3b380b-3fc6-4fb5-e341-51efc22b2191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second Chain: \n",
            "prompt=ChatPromptTemplate(input_variables=['english_review'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['english_review'], template='Summarize the review in 1 sentence: {english_review}'))]) llm=ChatGoogleGenerativeAI(model='models/gemini-pro', temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7f6f60078340>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x7f6f6007b6d0>, default_metadata=()) output_key='summary'\n",
            "Second chain input variables: \n",
            "['english_review']\n",
            "Second chain output variables: \n",
            "summary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What language is the following review:{review}\"\n",
        ")\n",
        "third_chain = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                       output_key=\"language\"\n",
        "                      )\n",
        "\n",
        "print(f\"Third Chain: \\n{third_chain}\")\n",
        "print(f\"Third chain input variables: \\n{third_chain.prompt.input_variables}\")\n",
        "print(f\"Third chain output variables: \\n{third_chain.output_key}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gL38xKR5iHn",
        "outputId": "78220fc1-9630-413f-f481-a83289ffec88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Third Chain: \n",
            "prompt=ChatPromptTemplate(input_variables=['review'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['review'], template='What language is the following review:{review}'))]) llm=ChatGoogleGenerativeAI(model='models/gemini-pro', temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7f6f60078340>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x7f6f6007b6d0>, default_metadata=()) output_key='language'\n",
            "Third chain input variables: \n",
            "['review']\n",
            "Third chain output variables: \n",
            "language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following \"\n",
        "    \"summary in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "# chain 4: input= summary, language and output= followup_message\n",
        "fourth_chain = LLMChain(llm=llm, prompt=fourth_prompt,\n",
        "                      output_key=\"followup_message\"\n",
        "                     )\n",
        "\n",
        "sequential_chain = SequentialChain(chains=[first_chain, second_chain, third_chain, fourth_chain],\n",
        "                                   input_variables=[\"review\"],\n",
        "                                   output_variables=[\"english_review\", \"summary\", \"followup_message\"],\n",
        "                                   verbose=True\n",
        "                                   )\n",
        "\n",
        "review = \"Il fait très chaud, c'est très rapide, \\\n",
        "c'est très drôle mais le colis n'était pas bon ce \\\n",
        "qui a abîmé une partie\"\n",
        "\n",
        "response = sequential_chain(review)\n",
        "\n",
        "print(f\"Review: \\n{response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp6uDZ2Z6BYB",
        "outputId": "cd314dc6-42c5-43d2-ea0a-60580843dff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Review: \n",
            "{'review': \"Il fait très chaud, c'est très rapide, c'est très drôle mais le colis n'était pas bon ce qui a abîmé une partie\", 'english_review': 'It is very hot, it is very fast, it is very funny but the package was not good which damaged a part', 'summary': \"Despite its impressive performance and humor, the product's packaging was inadequate, resulting in damage to a component.\", 'followup_message': \"Malgré ses performances impressionnantes et son humour, l'emballage du produit était inadéquat, ce qui a entraîné des dommages à un composant.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Router Chain"
      ],
      "metadata": {
        "id": "CEiTpTKq9fBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Router chain** is an advanced type of chain that includes decision making. It routes input to different chains or steps based on certain conditions."
      ],
      "metadata": {
        "id": "T7oHmK59IoWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is useful when different process has to be executed on type of user input or request"
      ],
      "metadata": {
        "id": "dGk5cymHI5PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "yY00aPT_ImUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "8xhc9F7I7prJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
        "                             temperature=0.6)\n",
        "\n",
        "# dictionary will store different LLM chains, each associated\n",
        "# with specific prompt name\n",
        "destination_chains = {}\n",
        "\n",
        "# prompt info is list of dictionaries\n",
        "for p_info in prompt_infos:\n",
        "  # extract the value associated with the key 'name'\n",
        "    name = p_info[\"name\"]\n",
        "  # extract value associated with the key 'prompt_template'\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "\n",
        "    # creates a prompt template\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "\n",
        "    # creates an instance of LLM chain and initialized with llm and prompt\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "    # stores the chain in the destination chain using prompt name as key\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "# list comprehension that iterates over each prompt info\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "\n",
        "# creates a simple prompt template that directly passes the input to the\n",
        "# language model without any special formatting\n",
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "metadata": {
        "id": "i2grOkWEJY21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates final template that will be used to guide the routing process\n",
        "\n",
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\"\n",
        "\n",
        "\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )\n"
      ],
      "metadata": {
        "id": "VWT6rp9WJzK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"What is artifical intelligence?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "0iw1deAPKBpi",
        "outputId": "2ae7aa1f-0fff-405e-9b10-c13d5e369ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "None: {'input': 'What is artifical intelligence?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**Artificial Intelligence (AI)**\\n\\nArtificial Intelligence (AI) is a field of computer science that aims to create machines or systems that can perform tasks typically requiring human intelligence, such as learning, problem-solving, decision-making, and natural language processing.\\n\\n**Key Characteristics of AI:**\\n\\n* **Learning:** AI systems can learn from data and improve their performance over time, without explicit programming.\\n* **Problem-Solving:** AI can analyze complex problems, identify patterns, and develop solutions.\\n* **Decision-Making:** AI systems can make decisions based on available information and learned models.\\n* **Natural Language Processing:** AI can understand, generate, and interpret human language effectively.\\n* **Adaptability:** AI systems can adapt to changing environments and learn from new experiences.\\n\\n**Types of AI:**\\n\\n* **Machine Learning:** AI systems that learn from data without explicit programming.\\n* **Deep Learning:** A subset of machine learning that uses artificial neural networks to learn complex patterns.\\n* **Natural Language Processing (NLP):** AI systems that analyze, understand, and generate human language.\\n* **Computer Vision:** AI systems that can interpret and analyze visual data.\\n* **Robotics:** AI systems that control and operate physical robots.\\n\\n**Applications of AI:**\\n\\nAI has a wide range of applications, including:\\n\\n* Healthcare: Diagnosis, treatment planning, and drug discovery\\n* Finance: Fraud detection, risk assessment, and investment management\\n* Transportation: Self-driving cars, traffic optimization, and logistics\\n* Manufacturing: Automation, quality control, and predictive maintenance\\n* Retail: Personalized recommendations, customer service, and inventory management'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"What is 3 + 3?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "9UAsbDcoKT0r",
        "outputId": "18bcc523-b1af-42fe-bb30-2e5ce4fe9bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "math: {'input': 'What is 3 + 3?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**Breaking down the problem:**\\n\\nThe problem \"What is 3 + 3?\" can be broken down into two component parts:\\n\\n1. What is 3?\\n2. What is 3?\\n\\n**Answering the component parts:**\\n\\n1. 3 is a number.\\n2. 3 is a number.\\n\\n**Putting the component parts together:**\\n\\nTo answer the broader question, we need to add the two numbers together:\\n\\n3 + 3 = 6\\n\\n**Therefore, the answer to the question \"What is 3 + 3?\" is 6.**'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents"
      ],
      "metadata": {
        "id": "4Pw-6f7-YRoR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e-Vql9spXuiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangGraph"
      ],
      "metadata": {
        "id": "32EAAmr7_n3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "result = llm.invoke(\"What is 2 + 2?\")\n",
        "print(f\"Response: {result.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLqNNMYg_sOr",
        "outputId": "d6f9daa2-dd02-4ade-cae5-0cb7445cabf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: 2 + 2 = 4 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self, system=\"\"): # constructor that takes a system message\n",
        "    self.system = system # system message is assigned to system variable\n",
        "    self.messages=[] # empty list to track all messages exchanged during converstaion\n",
        "    if self.system: # if system message is provided, it is added to the message list\n",
        "      self.messages.append({\"role\": \"system\", \"content\": system})\n",
        "\n",
        "  def __call__(self, message): # special method that allows an instance of Agent\n",
        "                  # class to be called like a function\n",
        "    self.messages.append({\"role\": \"user\", \"content\": message}) # the user message\n",
        "                        # is appended to the message list-\n",
        "    result = self.execute() # execute method used to get the response of message\n",
        "    self.messages.append({\"role\": \"assistant\", \"content\": result}) # response is appended to message list\n",
        "    return result\n",
        "\n",
        "  def execute(self): # function that is called to execute the message list to get\n",
        "                  # a response\n",
        "    response = llm.invoke(self.messages)\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "zRKXQkdbCJTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "At the end of the loop you output an Answer\n",
        "Use Thought to describe your thoughts about the question you have been asked.\n",
        "Use Action to run one of the actions available to you - then return PAUSE.\n",
        "Observation will be the result of running those actions.\n",
        "\n",
        "Your available actions are:\n",
        "\n",
        "calculate:\n",
        "e.g. calculate: 4 * 7 / 3\n",
        "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
        "\n",
        "average_dog_weight:\n",
        "e.g. average_dog_weight: Collie\n",
        "returns average weight of a dog when given the breed\n",
        "\n",
        "Example session:\n",
        "\n",
        "Question: How much does a Bulldog weigh?\n",
        "Thought: I should look the dogs weight using average_dog_weight\n",
        "Action: average_dog_weight: Bulldog\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: A Bulldog weights 51 lbs\n",
        "\n",
        "You then output:\n",
        "\n",
        "Answer: A bulldog weights 51 lbs\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "7DEem1AZDzRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate(attribute): # takes a string as input and uses the `eval()` function\n",
        "              # to evaluate it as Python expression\n",
        "    return eval(attribute) #eval function executes the attribute as python expression\n",
        "          # if attribute = '5 + 2', the response will be 7\n",
        "\n",
        "def average_dog_weight(name): # function that takes a name and return a reponse\n",
        "                    # based on that name\n",
        "    if name in \"Scottish Terrier\":\n",
        "        return(\"Scottish Terriers average 20 lbs\")\n",
        "    elif name in \"Border Collie\":\n",
        "        return(\"a Border Collies average weight is 37 lbs\")\n",
        "    elif name in \"Toy Poodle\":\n",
        "        return(\"a toy poodles average weight is 7 lbs\")\n",
        "    else:\n",
        "        return(\"An average dog weights 50 lbs\")\n",
        "\n",
        "known_actions = { # maps strings to corresponding function objects. This allows\n",
        "                  # for dynamic function calling\n",
        "    \"calculate\": calculate,\n",
        "    \"average_dog_weight\": average_dog_weight\n",
        "}"
      ],
      "metadata": {
        "id": "pq2yDGQxD1Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(prompt) # prompt is passed as a system message\n",
        "agent # you can see that the data type is Agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS16slY1EsP6",
        "outputId": "5329a102-b8ca-483c-c339-645ae4767826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Agent at 0x7c5f56958fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent(\"How much does a Border Collie weigh?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1w4-LE0Ez-0",
        "outputId": "6a40511f-847c-474b-d6f8-9b7b6bc34743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I should look up the average weight of a Border Collie.\n",
            "Action: average_dog_weight: Border Collie\n",
            "PAUSE \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = average_dog_weight(\"Border Collie\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvudZfihE52V",
        "outputId": "b07e0dce-aee5-424d-9563-ea75c8b7876a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a Border Collies average weight is 37 lbs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_prompt = f\"Observation: {result}\"\n",
        "agent(next_prompt)\n",
        "agent.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsTsJh2IFs2k",
        "outputId": "b7797df5-f63e-405e-e61c-8a9a781bef67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': 'You run in a loop of Thought, Action, PAUSE, Observation.\\nAt the end of the loop you output an Answer\\nUse Thought to describe your thoughts about the question you have been asked.\\nUse Action to run one of the actions available to you - then return PAUSE.\\nObservation will be the result of running those actions.\\n\\nYour available actions are:\\n\\ncalculate:\\ne.g. calculate: 4 * 7 / 3\\nRuns a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\\n\\naverage_dog_weight:\\ne.g. average_dog_weight: Collie\\nreturns average weight of a dog when given the breed\\n\\nExample session:\\n\\nQuestion: How much does a Bulldog weigh?\\nThought: I should look the dogs weight using average_dog_weight\\nAction: average_dog_weight: Bulldog\\nPAUSE\\n\\nYou will be called again with this:\\n\\nObservation: A Bulldog weights 51 lbs\\n\\nYou then output:\\n\\nAnswer: A bulldog weights 51 lbs'},\n",
              " {'role': 'user', 'content': 'How much does a Border Collie weigh?'},\n",
              " {'role': 'user', 'content': 'How much does a Border Collie weigh?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Thought: I should look up the average weight of a Border Collie.\\nAction: average_dog_weight: Border Collie\\nPAUSE \\n'},\n",
              " {'role': 'user',\n",
              "  'content': 'Observation: a Border Collies average weight is 37 lbs'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Answer: A Border Collie weighs 37 lbs on average. \\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agents with Langgraph"
      ],
      "metadata": {
        "id": "4nnSjs2f526L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool  = TavilySearchResults(max_results=2) # we will get only two responses from\n",
        "                      # the search API\n",
        "print(type(tool))\n",
        "print(tool.name) # the name that the language model will use to call this tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFKivlGwF58V",
        "outputId": "b37b538e-cecd-4244-c8a6-1249721c8b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'>\n",
            "tavily_search_results_json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict): # state of the agent that it will preserve throughout\n",
        "                            # the workflow\n",
        "  messages: Annotated[list[AnyMessage], operator.add] # Annotated allows the\n",
        "  # developer to declare type of a reference and additional information related\n",
        "  # to it\n",
        "\n",
        "  # Annotated list of messages that we will add over time"
      ],
      "metadata": {
        "id": "DVnhL7Wb6LLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  # Three functions required.\n",
        "  # 1. To call the language model\n",
        "  # 2. Check whether an action is present\n",
        "  # 3. To take that action\n",
        "\n",
        "  def __init__(self, model, tools, system=\"\"):\n",
        "    self.system = system\n",
        "    # creating the graph\n",
        "    graph = StateGraph(AgentState)\n",
        "    graph.add_node(\"llm\",  self.call_llm) # add node to the graph and pass function that\n",
        "                            # we want to represent this node\n",
        "    graph.add_node(\"action\", self.take_action)\n",
        "    graph.add_conditional_edges(\n",
        "        \"llm\", # where llm starts\n",
        "        self.exists_action, # function that will determine where to go after that\n",
        "        {True: \"action\", False: END} # dictionary representing how to map the response\n",
        "                                  # of the fuction to the next node to go to\n",
        "                                  # if the function returns true, we will go to\n",
        "                                  # action node, else we will go to end node\n",
        "    )\n",
        "    graph.add_edge(\"action\", \"llm\")\n",
        "    graph.set_entry_point(\"llm\")\n",
        "    self.graph = graph.compile()\n",
        "    self.tools = {t.name: t for t in tools} # create dictionary mapping the name\n",
        "                                    # of the tool to the tool itself\n",
        "    self.model = model.bind_tools(tools) # letting the model know that it has these\n",
        "                              # tools available to call\n",
        "\n",
        "  def exists_action(self, state: AgentState):\n",
        "    result = state['messages'][-1]\n",
        "    return len(result.tool_calls) > 0\n",
        "\n",
        "  def call_llm(self, state: AgentState):\n",
        "    messages = state['messages']\n",
        "    if self.system:\n",
        "      messages = [SystemMessage(content=self.system)] + messages\n",
        "    message = self.model.invoke(messages)\n",
        "    return {'messages': [message]}\n",
        "\n",
        "  def take_action(self, state: AgentState): # action node\n",
        "    tool_calls = state['messages'][-1].tool_calls\n",
        "    results = []\n",
        "    for t in tool_calls:\n",
        "      print(f\"Calling: {t}\")\n",
        "      result = self.tools[t['name']].invoke(t['args'])\n",
        "      results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
        "\n",
        "    print(\"Back to the model!\")\n",
        "    return {'messages': results}"
      ],
      "metadata": {
        "id": "t7M5HR9sP2uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
        "You are allowed to make multiple calls (either together or in sequence). \\\n",
        "Only look up information when you are sure of what you want. \\\n",
        "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
        "\"\"\"\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "agent = Agent(model, [tool], prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_JxAbrsj2PH",
        "outputId": "c5380bcc-803f-498a-fceb-288f624bb10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n",
            "WARNING:langchain_google_genai._function_utils:Key 'title' is not supported in schema, ignoring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"What is the weather in Peshawar\")]\n",
        "result = agent.graph.invoke({\"messages\": messages})\n",
        "print(result['messages'][-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyGMLqylkDUp",
        "outputId": "0fe7115d-8ecc-4052-8645-dcc205daf7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'weather in Peshawar'}, 'id': 'ec18b40c-761b-48ce-a20e-9de9af04fe1b', 'type': 'tool_call'}\n",
            "Back to the model!\n",
            "The weather in Peshawar, Pakistan is currently 25.2 degrees Celsius and misty. The wind is blowing from the southeast at 15.1 kilometers per hour. The humidity is 83%. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"What is the weather in Peshawar and Taru Jabba?\")]\n",
        "result = agent.graph.invoke({\"messages\": messages})\n",
        "print(result['messages'][-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfmxTGTjkUr5",
        "outputId": "77ede67a-d35a-4f1e-d0d3-ac0009ea998d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'weather in Peshawar and Taru Jabba'}, 'id': 'eb38e6a2-dfb1-4923-846d-5a803943f796', 'type': 'tool_call'}\n",
            "Back to the model!\n",
            "I can't find the weather for Taru Jabba, but the weather in Peshawar is currently 10.9 degrees Celsius with patchy rain nearby. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agentic Search"
      ],
      "metadata": {
        "id": "kjkBMyGPuJYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_client = TavilyClient(api_key = os.environ.get(\"TAVILY_API_KEY\"))\n",
        "result = tavily_client.search('What is the weather in Madrid', include_answer = True)\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdx3xxkIl8HG",
        "outputId": "b17eab8d-cf60-4104-d72e-d7e13bb7a019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current weather in Madrid is partly cloudy with a temperature of 30.3°C (86.5°F). The wind is coming from the NNE direction at 6.8 kph (4.3 mph), and the humidity is at 46%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple searching"
      ],
      "metadata": {
        "id": "GX6O7xLUw4hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "city = \"Madrid\"\n",
        "\n",
        "query = f\"\"\"\n",
        "What is the current weather in {city}?\n",
        "Should I travel there today for a picnic?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IxsCUEVRvqqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddg = DDGS()\n",
        "\n",
        "def search(query, max_results=3):\n",
        "  results = ddg.text(query, max_results=max_results)\n",
        "  return [i[\"href\"] for i in results]\n",
        "\n",
        "for i in search(query):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2grMNgnvppp",
        "outputId": "4e0ae30b-b573-4ef3-d606-c704dcf567ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.accuweather.com/en/es/madrid/308526/current-weather/308526\n",
            "https://www.accuweather.com/en/es/madrid/308526/air-travel-weather/308526\n",
            "https://www.accuweather.com/en/es/madrid/308526/weather-forecast/308526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_weather_info(url):\n",
        "  if not url:\n",
        "    return \"Weather information could not be found\"\n",
        "\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "  response = requests.get(url, headers=headers)\n",
        "  if response.status_code != 200:\n",
        "    return \"Failed to retrieve the webpage\"\n",
        "\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "  return soup\n",
        "\n",
        "url = search(query)[0]\n",
        "soup = scrape_weather_info(url)\n",
        "\n",
        "# extract text\n",
        "weather_data = []\n",
        "for tag in soup.find_all(['h1', 'h2', 'h3', 'p']):\n",
        "    text = tag.get_text(\" \", strip=True)\n",
        "    weather_data.append(text)\n",
        "\n",
        "# combine all elements into a single string\n",
        "weather_data = \"\\n\".join(weather_data)\n",
        "\n",
        "# remove all spaces from the combined text\n",
        "weather_data = re.sub(r'\\s+', ' ', weather_data)\n",
        "\n",
        "print(f\"Website: {url}\\n\\n\")\n",
        "print(weather_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Cj9AG9wyqs8",
        "outputId": "c61f55f3-3f6b-48ba-b4cb-12520b25bb18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Website: https://www.accuweather.com/en/es/madrid/308526/current-weather/308526\n",
            "\n",
            "\n",
            "Madrid, Madrid Madrid Madrid Around the Globe Around the Globe Hurricane Tracker Severe Weather Radar & Maps News & Features Astronomy Business Climate Health Recreation Sports Travel Video Current Weather 5:02 PM Day Yellow Warning for Storms 12:00 PM Friday - 12:00 AM Saturday Yellow Warning for Rain 12:00 PM Friday - 12:00 AM Saturday Max UV Index 5 Moderate Wind ENE 5 mph Wind Gusts 12 mph Probability of Precipitation 80% Probability of Thunderstorms 32% Precipitation 0.02 in Rain 0.02 in Hours of Precipitation 3 Hours of Rain 3 Cloud Cover 77% Morning Afternoon Night Yellow Warning for Storms 12:00 PM Friday - 12:00 AM Saturday Yellow Warning for Rain 12:00 PM Friday - 12:00 AM Saturday Wind ESE 5 mph Wind Gusts 14 mph Probability of Precipitation 40% Probability of Thunderstorms 24% Precipitation 0.05 in Rain 0.05 in Hours of Precipitation 1.5 Hours of Rain 1.5 Cloud Cover 38% Evening Overnight Sun & Moon Temperature History Further Ahead Further Ahead Hourly Daily Monthly Around the Globe Around the Globe Hurricane Tracker Severe Weather Radar & Maps News Video Top Stories Severe Weather Severe weather to target Chicago, Detroit before the weekend 1 hour ago Travel 7 people injured after turbulence forces United Airlines flight land 1 day ago Weather Forecasts Labor Day weather: Where storms will spoil unofficial end of summer 6 hours ago Hurricane \"Everything was on fire or under water\": Katrina hero recalls rescues 23 hours ago Weather Forecasts Refreshing change ahead for the Midwest and Northeast 20 hours ago Featured Stories Recreation American father and daughter recall horror of Iceland cave collapse 1 day ago Health Oropouche virus: Emerging threat raises concern among health officials 1 day ago Health Coating clothes with a simple material could cool your body 2 days ago Case Studies Prehistoric sea cow was eaten by a croc and a shark, newly discovered ... 49 minutes ago Weather News Millions in this country are stranded by flooding 2 days ago Weather Near Madrid: We have updated our Privacy Policy and Cookie Policy . Get AccuWeather alerts as they happen with our browser notifications. Notifications Enabled Thanks! We’ll keep you informed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agentic Search"
      ],
      "metadata": {
        "id": "bA1CpSfjzyxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = tavily_client.search(query, max_results=1)\n",
        "data = result[\"results\"][0][\"content\"]\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWAoHhv5zUEM",
        "outputId": "64ec5d18-a5f9-4ebc-e118-4de99b91088b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'location': {'name': 'Madrid', 'region': 'Madrid', 'country': 'Spain', 'lat': 40.4, 'lon': -3.68, 'tz_id': 'Europe/Madrid', 'localtime_epoch': 1725030221, 'localtime': '2024-08-30 17:03'}, 'current': {'last_updated_epoch': 1725030000, 'last_updated': '2024-08-30 17:00', 'temp_c': 30.1, 'temp_f': 86.2, 'is_day': 1, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 2.5, 'wind_kph': 4.0, 'wind_degree': 70, 'wind_dir': 'ENE', 'pressure_mb': 1015.0, 'pressure_in': 29.97, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 46, 'cloud': 50, 'feelslike_c': 28.8, 'feelslike_f': 83.9, 'windchill_c': 31.9, 'windchill_f': 89.5, 'heatindex_c': 30.9, 'heatindex_f': 87.6, 'dewpoint_c': 11.5, 'dewpoint_f': 52.6, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 8.0, 'gust_mph': 6.7, 'gust_kph': 10.8}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pygments import highlight, lexers, formatters\n",
        "\n",
        "# parse JSON\n",
        "parsed_json = json.loads(data.replace(\"'\", '\"'))\n",
        "\n",
        "# pretty print JSON with syntax highlighting\n",
        "formatted_json = json.dumps(parsed_json, indent=4)\n",
        "colorful_json = highlight(formatted_json,\n",
        "                          lexers.JsonLexer(),\n",
        "                          formatters.TerminalFormatter())\n",
        "\n",
        "print(colorful_json)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DNYw5CI0Ec7",
        "outputId": "8a44ac68-4c7c-43a1-b032-5816bdc1ec1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[94m\"location\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"name\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Madrid\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"region\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Madrid\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"country\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Spain\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"lat\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m40.4\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"lon\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m-3.68\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"tz_id\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Europe/Madrid\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"localtime_epoch\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m1725030221\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"localtime\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"2024-08-30 17:03\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[94m\"current\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"last_updated_epoch\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m1725030000\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"last_updated\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"2024-08-30 17:00\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"temp_c\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m30.1\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"temp_f\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m86.2\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"is_day\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m1\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"condition\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m{\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m            \u001b[39;49;00m\u001b[94m\"text\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"Partly cloudy\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m            \u001b[39;49;00m\u001b[94m\"icon\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"//cdn.weatherapi.com/weather/64x64/day/116.png\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m            \u001b[39;49;00m\u001b[94m\"code\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m1003\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m},\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"wind_mph\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m2.5\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"wind_kph\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m4.0\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"wind_degree\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m70\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"wind_dir\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[33m\"ENE\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"pressure_mb\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m1015.0\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"pressure_in\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m29.97\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"precip_mm\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m0.0\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"precip_in\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m0.0\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"humidity\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m46\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"cloud\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m50\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"feelslike_c\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m28.8\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"feelslike_f\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m83.9\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"windchill_c\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m31.9\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"windchill_f\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m89.5\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"heatindex_c\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m30.9\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"heatindex_f\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m87.6\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"dewpoint_c\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m11.5\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"dewpoint_f\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m52.6\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"vis_km\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m10.0\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"vis_miles\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m6.0\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"uv\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m8.0\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"gust_mph\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m6.7\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m\u001b[94m\"gust_kph\"\u001b[39;49;00m:\u001b[37m \u001b[39;49;00m\u001b[34m10.8\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m}\u001b[37m\u001b[39;49;00m\n",
            "}\u001b[37m\u001b[39;49;00m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Persistence and Streaming in Agents"
      ],
      "metadata": {
        "id": "wo6e8G5hEe5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool = TavilySearchResults(max_results=2)\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], operator.add]"
      ],
      "metadata": {
        "id": "tsxgSRc20PWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = SqliteSaver.from_conn_string(\":memory:\")"
      ],
      "metadata": {
        "id": "99sN_Ad6Gu_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
        "        self.system = system\n",
        "        graph = StateGraph(AgentState)\n",
        "        graph.add_node(\"llm\", self.call_llm)\n",
        "        graph.add_node(\"action\", self.take_action)\n",
        "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
        "        graph.add_edge(\"action\", \"llm\")\n",
        "        graph.set_entry_point(\"llm\")\n",
        "        self.graph = graph.compile(checkpointer=checkpointer)\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "        self.model = model.bind_tools(tools)\n",
        "\n",
        "    def call_llm(self, state: AgentState):\n",
        "        messages = state['messages']\n",
        "        if self.system:\n",
        "            messages = [SystemMessage(content=self.system)] + messages\n",
        "        message = self.model.invoke(messages)\n",
        "        return {'messages': [message]}\n",
        "\n",
        "    def exists_action(self, state: AgentState):\n",
        "        result = state['messages'][-1]\n",
        "        return len(result.tool_calls) > 0\n",
        "\n",
        "    def take_action(self, state: AgentState):\n",
        "        tool_calls = state['messages'][-1].tool_calls\n",
        "        results = []\n",
        "        for t in tool_calls:\n",
        "            print(f\"Calling: {t}\")\n",
        "            result = self.tools[t['name']].invoke(t['args'])\n",
        "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
        "        print(\"Back to the model!\")\n",
        "        return {'messages': results}"
      ],
      "metadata": {
        "id": "lHqZF8UgE0LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
        "You are allowed to make multiple calls (either together or in sequence). \\\n",
        "Only look up information when you are sure of what you want. \\\n",
        "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
        "\"\"\"\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "PCeK6wT7FfAx",
        "outputId": "4de20630-6d32-4029-ee64-acd021e8e3c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for CompiledStateGraph\ncheckpointer\n  instance of BaseCheckpointSaver expected (type=type_error.arbitrary_type; expected_arbitrary_type=BaseCheckpointSaver)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4cddb6ab5f22>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatGoogleGenerativeAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gemini-1.5-flash\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mabot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpointer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-cd0cf7e29ffa>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tools, checkpointer, system)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"action\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"llm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_entry_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"llm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, checkpointer, store, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    437\u001b[0m         )\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         compiled = CompiledStateGraph(\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0mbuilder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mconfig_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__dict__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for CompiledStateGraph\ncheckpointer\n  instance of BaseCheckpointSaver expected (type=type_error.arbitrary_type; expected_arbitrary_type=BaseCheckpointSaver)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IoQ54r0dKOgM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}