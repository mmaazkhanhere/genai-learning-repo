{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "Pwmec_bO-dDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baab24b0-883c-4d45-c28f-457c955a0b50"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API Key\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh3fGEYTLXtR",
        "outputId": "ae6beab3-95f9-4c30-895f-a33ff80327ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provide your Google API Key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "YPibOxplMTY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Langchain** is a framework for developing applications powered by large language model. It is an open source ochestration framework for the development of applications that uses LLM"
      ],
      "metadata": {
        "id": "96Ken1_y_qsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a generic interface for any LLM, abstracting the complexities of interacting with different LLMs"
      ],
      "metadata": {
        "id": "9rRLkqptMkV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chains, in langchain, are core of the langchain work flow. It combine LLMs with other components, creating applications by executing a sequence of functions"
      ],
      "metadata": {
        "id": "hNceZ00xM03S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Models"
      ],
      "metadata": {
        "id": "wuYhB2dOMuFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chat models** is a language model that uses chat messages as input and return chat messages as outputs."
      ],
      "metadata": {
        "id": "HuAZrfKBMwPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "result = llm.invoke(\"What is 2 + 2?\")\n",
        "print(f\"Response: {result.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwyi1sHrMhVL",
        "outputId": "c327efe1-6aec-4213-caf2-d5f2640e1b06"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Template\n",
        "It takes a prompt as input, which is set of instructions or input provided by user to guie model's response. It helps generate relevant output and understanding the context"
      ],
      "metadata": {
        "id": "UBCOOWvwPrew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt template** class formalizes the composition of prompts without the need to manually hard code context and queries"
      ],
      "metadata": {
        "id": "CfDxqcFZP20E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Translate the text delimited by triple back ticks into\n",
        "style that is {style}\n",
        "\n",
        "text: ```{text}```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IwbOvz-bQmXF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "print(f\"Prompt template passed to the model:\\n{prompt_template.messages[0].prompt.template}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFQwZHafN6i_",
        "outputId": "695d1c6f-60db-44ab-d84d-965c9676d243"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt template passed to the model:\n",
            " \n",
            "Translate the text delimited by triple back ticks into\n",
            "style that is {style}\n",
            "\n",
            "text: ```{text}```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Inputs in the prompt: {prompt_template.messages[0].prompt.input_variables}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y19U0aVsReWm",
        "outputId": "d7981ae9-4966-454f-8c84-f71535fe68a4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs in the prompt: ['style', 'text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_email = \"Yeh kya hae bhai? Yeh woh cheez nahi hae jo mein nein \\\n",
        "order ki thi. Tasweer mein yeh buhat muhktalif tha. Abb mein kya karoon? \\\n",
        "Replacment or return ki kya policy hae?\"\n",
        "\n",
        "style = \"American style that is polite, calm and respectful in tone\""
      ],
      "metadata": {
        "id": "8W19FkzRQ_Ht"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_message = prompt_template.format_messages(style=style,\n",
        "                                                   text=customer_email)\n",
        "\n",
        "print(f\"Customer message: {customer_message[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrLkAV8ARDXO",
        "outputId": "70a41c8c-e336-4e62-90cc-2fd6deafe807"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer message: content=' \\nTranslate the text delimited by triple back ticks into\\nstyle that is American style that is polite, calm and respectful in tone\\n\\ntext: ```Yeh kya hae bhai? Yeh woh cheez nahi hae jo mein nein order ki thi. Tasweer mein yeh buhat muhktalif tha. Abb mein kya karoon? Replacment or return ki kya policy hae?```\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(customer_message)\n",
        "print(f\"Model response: {response.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkK37WxRR9eA",
        "outputId": "80adcf11-a34e-40a6-8de2-c2d29da6ccb9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model response: Excuse me, this is not what I ordered. It looks very different from the picture. What should I do now? What is your policy on replacements or returns?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_reply = \"\"\"\n",
        "Thanks for reaching out. We are sorry for inconvenience. \\\n",
        "We upload high quality pictures that best reflects the \\\n",
        "product quality. You can replace the item if it is ordered \\\n",
        "within 7 days. You cannot return the item. Our team will check \\\n",
        "the item and tell you what to do next. Thanks for understanding\n",
        "\"\"\"\n",
        "\n",
        "service_language = \"\"\"\n",
        "Translate the text into Urdu words using polite\n",
        "tone.\n",
        "\"\"\"\n",
        "\n",
        "service_message = prompt_template.format_messages(style=service_language,\n",
        "                                                  text=service_reply)\n",
        "\n",
        "\n",
        "service_response = llm.invoke(service_message)\n",
        "print(service_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqyqTogESOqC",
        "outputId": "2ab7d73a-e6df-492f-ca22-4e548bdd9048"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "شکریہ کہ ہمارے پاس پہنچے۔ ہم آپ کو ہونے والی پریشانی پر معذرت خواہ ہیں۔ ہم بہترین معیار کی تصاویر اپ لوڈ کرتے ہیں جو مصنوعات کے معیار کی عکاسی کرتی ہیں۔ اگر آپ نے 7 دنوں کے اندر آرڈر کیا ہے تو آپ اس آئٹم کو تبدیل کر سکتے ہیں۔ آپ اس آئٹم کو واپس نہیں کر سکتے۔ ہماری ٹیم آئٹم چیک کرے گی اور آپ کو بتائے گی کہ اس کے بعد کیا کرنا ہے۔ سمجھنے کے لئے شکریہ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see from the above example that instead of creating template for each response, we can create a template and reuse it"
      ],
      "metadata": {
        "id": "xlCEhM3oT5m2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory\n",
        "Traditionally, large language models (LLMS) doesn't have any long term memory of prior conversation unless you pass the whole chat history as an input. Langchain solve this problem with simple utilities for adding in memory into your application"
      ],
      "metadata": {
        "id": "CKlN72RuYHZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we alk about memory in langchain, we are referring to how the system remember the information from previous interactions"
      ],
      "metadata": {
        "id": "fEZU-a7uYpkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user = \"Hello! My name is Maaz\"\n",
        "response = llm.invoke(user)\n",
        "print(f\"User: {user}\\nResponse: {response.content}\")\n",
        "\n",
        "user = \"What is 2 + 2?\"\n",
        "response = llm.invoke(user)\n",
        "print(f\"User: {user}\\nResponse: {response.content}\")\n",
        "\n",
        "user = \"What is my name?\"\n",
        "response = llm.invoke(user)\n",
        "print(f\"User: {user}\\nResponse: {response.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WmKSxTtZDMe",
        "outputId": "5c1eec01-1583-47c3-e95a-1b9fd3313104"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hello! My name is Maaz\n",
            "Response: Hello Maaz! It's great to meet you. I am Gemini, a multimodal AI language model. I am designed to understand and generate human language, and to answer your questions and assist you with a wide range of tasks. Is there anything I can help you with today?\n",
            "User: What is 2 + 2?\n",
            "Response: 4\n",
            "User: What is my name?\n",
            "Response: I do not have access to your personal information, including your name, as I am a chatbot assistant and do not have a physical presence or the ability to interact with the real world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the model didn't remember my name despite being recently shared. The response of LLM model is stateless"
      ],
      "metadata": {
        "id": "hHLNjNJcZpu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Buffer Memory"
      ],
      "metadata": {
        "id": "MfEssywvYytM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It stores the entire conversation history from beginning to end"
      ],
      "metadata": {
        "id": "As62gV_qZz0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "conversation.predict(input = \"My name is Maaz\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "iHmfw6MzTCC9",
        "outputId": "ba6da1d4-c1c1-442b-dbb6-32c331aa37b8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: My name is Maaz\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Maaz, my name is Gemini. How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is 2 + 2?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "vEk8H5QSarBx",
        "outputId": "55631cd8-f8de-46ae-c6c2-8363cd411491"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: My name is Maaz\n",
            "AI: Hello Maaz, my name is Gemini. How can I help you today?\n",
            "Human: What is 2 + 2?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2 + 2 is 4.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "rCieNyEya5GT",
        "outputId": "6f81dd75-aba8-4c9a-cda4-0f680ae28ab1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: My name is Maaz\n",
            "AI: Hello Maaz, my name is Gemini. How can I help you today?\n",
            "Human: What is 2 + 2?\n",
            "AI: 2 + 2 is 4.\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Maaz, as you told me earlier.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Entire conversation: \\n{memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJDhwgBKbqdL",
        "outputId": "6bca9698-3b2c-4ad4-d76f-55eb9bd52028"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entire conversation: \n",
            "Human: My name is Maaz\n",
            "AI: Hello Maaz, my name is Gemini. How can I help you today?\n",
            "Human: What is 2 + 2?\n",
            "AI: 2 + 2 is 4.\n",
            "Human: What is my name?\n",
            "AI: Your name is Maaz, as you told me earlier.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAPWeQptb2F-",
        "outputId": "0e510c2c-1e00-416f-8a98-3dcd6ac73cae"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: My name is Maaz\\nAI: Hello Maaz, my name is Gemini. How can I help you today?\\nHuman: What is 2 + 2?\\nAI: 2 + 2 is 4.\\nHuman: What is my name?\\nAI: Your name is Maaz, as you told me earlier.'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Buffer Window Memory"
      ],
      "metadata": {
        "id": "TPkrG0EkbF46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It stores only the most recent exchanges within defined window size. Instead of saving entire conversation, it focuses on recent conversation"
      ],
      "metadata": {
        "id": "ySvKf83hbMr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is useful when the goal is to make AI focus on recent chat without overwheling by too much information"
      ],
      "metadata": {
        "id": "G2zaqSjcbdo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=1) #remember only 1 recent message\n",
        "\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "\n",
        "print(f\"Conversations: \\n{memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FInsACIKa_ux",
        "outputId": "13333a66-e0b4-49e1-ce74-2ac57aabb734"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversations: \n",
            "Human: Not much, just hanging\n",
            "AI: Cool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "memory = ConversationBufferWindowMemory(k=2) #remember only 2 recent message\n",
        "\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"How is the weather\"},\n",
        "                    {\"output\": \"Hot\"})\n",
        "\n",
        "print(f\"Conversations: \\n{memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgzczFJ8chWG",
        "outputId": "3561b61d-cc75-4b9e-e8ea-4efe9d687484"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversations: \n",
            "Human: Not much, just hanging\n",
            "AI: Cool\n",
            "Human: How is the weather\n",
            "AI: Hot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Token Buffer Memory"
      ],
      "metadata": {
        "id": "ySUXIGpScyHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the conversation buffer memory but instead of counting interactions, it count tokens"
      ],
      "metadata": {
        "id": "9fXiU-frc36p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This memory type limits how much the AI remembers based on number of tokens. Once the token limit is reached, the oldest tokens are forgotten"
      ],
      "metadata": {
        "id": "7y5jM0QTdCCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "\n",
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=40)\n",
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})\n",
        "\n",
        "print(f\"Conversations: {memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLvzwJmzcpV3",
        "outputId": "4ba63da7-4753-4d12-f038-b1ba8d9d3b90"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversations: Human: AI is what?!\n",
            "AI: Amazing!\n",
            "Human: Backpropagation is what?\n",
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"How is the weather?\"},\n",
        "                    {\"output\": \"Windy and cold!\"})\n",
        "print(f\"Memory: \\n{memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbPleG1OdOQS",
        "outputId": "a395ba4d-f27d-4694-8fbe-aca82897af1f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory: \n",
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n",
            "Human: How is the weather?\n",
            "AI: Windy!\n",
            "Human: How is the weather?\n",
            "AI: Windy and cold!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Summary Memory"
      ],
      "metadata": {
        "id": "Sfx59OHKk0xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It doesn't store every detail but creates a summary of past interactions. This allows the model to remember what is important without needing full conversation"
      ],
      "metadata": {
        "id": "qgnnXVR6k6vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})\n",
        "\n",
        "print(f\"Conversation in memory: \\n{memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap7zD2BidseF",
        "outputId": "5a01dc6d-72d1-443a-9f77-c8e693441dce"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversation in memory: \n",
            "System: The human says hello. The AI asks what's up. The human says they are hanging. The AI says that's cool. The human asks what is on the schedule today.\n",
            "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(llm=llm,\n",
        "                                 memory=memory,\n",
        "                                 verbose=True)\n",
        "\n",
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "rsOHsKcHlJpl",
        "outputId": "d5c952cd-7b53-4e25-94c9-f53afc6884c4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human says hello. The AI asks what's up. The human says they are hanging. The AI says that's cool. The human asks what is on the schedule today.\n",
            "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
            "Human: What would be a good demo to show?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I recommend the latest demo for LLM that allows the model to generate text in the voice of a specific person, including celebrities.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"New memory: {memory.buffer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j6-7YKElpl5",
        "outputId": "535cbe9c-071d-4b57-94ee-cbbf31eba39c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New memory: System: The human says hello. The AI asks what's up. The human says they are hanging. The AI says that's cool. The human asks what is on the schedule today. The AI says there is a meeting at 8am with the product team, work on the LangChain project from 9am-12pm, and lunch at an Italian restaurant with a customer at Noon.\n",
            "Human: What would be a good demo to show?\n",
            "AI: I recommend the latest demo for LLM that allows the model to generate text in the voice of a specific person, including celebrities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ALgA86nylszy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}